{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hidden Markov Chain Text Segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6JO44PNDEFt/+HkwszalF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQB4dpwmUi0G"
      },
      "source": [
        "#TP 1 : LE MODELE HMC POUR LA SEGMENTATION DE TEXTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9NcNXdkj8V5"
      },
      "source": [
        "Nom: DOUCOURE\n",
        "\n",
        "Prénom: Dioula"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG59vojD7L-F"
      },
      "source": [
        "##**Question 1**: Démontrez que les fonctions forward et backward se calculent bien "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTUSS5mPE85C"
      },
      "source": [
        "Notation: Probabilité d'un état: $\\pi=P(X_t=i)$ \\\\\n",
        "Probabilité de transition $a_{ij}=P(X_t=j|X_{t-1}=i)$ \\\\\n",
        "Probabilité d'emission: $b_i(y)=P(Y_t=y|X_t=i)$\n",
        "\n",
        "**Forward** \\\\\n",
        "On a $\\alpha_i(t) =P(X_t=i,y_{1:t})$ \\\\\n",
        "Pour $t=1$, on peut écrire:\n",
        "\n",
        "\\begin{align} \n",
        "\\alpha_i(1) &= P(X_1= i,y_1) \\\\ \n",
        "                   &= P(y_1|X_1=i)P(X_1=i) \\\\ \n",
        "                   &= P(X_t=i) P(y_1|X_t=i) \\quad \\quad (X_t \\quad uniforme)\\\\ \n",
        "                   &= \\pi_i b_{i}(y_1) \\\\ \n",
        "\\end{align}                   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Déterminins $\\alpha_i(t+1)$ pour $t\\in \\{2,\\dots, T-2\\}$, on a:\n",
        "\n",
        "\\begin{align} \n",
        "\\alpha_i(t+1) &= P \\Big( X_{t+1}=i, y_{1:t+1} \\Big)  \\\\ \n",
        "                   &={\\sum_{j=1}^N} P\\Big(X_{t+1}=i,X_t=j,\\quad y_{1:t+1} \\Big) \\\\ \n",
        "                   &=  \\sum_{j=1}^N  P\\Big(y_{t+1} | X_{t+1}=i, y_{1:t},X_t= j\\Big) \n",
        "\t                P\\Big(X_{t+1}=i, y_{1:t},X_t=j \\Big) \\\\ \n",
        "                   &=  \\sum_{j=1}^N  P\\Big(y_{t+1} | X_{t+1}= i, y_{1:t}, X_t= j\\Big) \n",
        "\t                P\\Big(X_{t+1}=i | y_{1:t}, X_t=j\\Big) P\\Big(X_t=j, y_{1:t}\\Big)\\\\ \n",
        "                   &=  \\sum_{j=1}^N  P\\Big(y_{t+1} | X_{t+1}= i\\Big) P\\Big(X_{t+1}=i | X_t=j\\Big) p\\Big(X_t=j, y_{1:t}\\Big)\\\\ \n",
        "                   &= P\\Big(y_{t+1} | X_{t+1}=i\\Big) \\sum_{j=1}^N P\\Big(X_{t+1}=i | X_t=j\\Big) P\\Big(X_t=j, y_{1:t}\\Big) \\\\ \n",
        "                   &= b_{i}(y_{t+1})\\sum_{j=1}^N a_{ji} \\alpha_i(t)\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXz8O_8tcNJs"
      },
      "source": [
        "**Backward** \\\\\n",
        "Avec les mêmes notations que pour le forward, on a:\n",
        "\n",
        "\\begin{align} \n",
        "\\beta_i(t) &= P \\Big( y_{t+1:T} | X_t = i \\Big) \\\\ \n",
        "&= \\sum_{j=1}^T P\\Big( y_{t+1:T}, X_{t+1}= j | X_t = i  \\Big)        \\\\ \n",
        "&= \\sum_{j=1}^T P\\Big( y_{t+2:T} |  y_{t+1} , X_{t+1} = j , X_t = i  \\Big) \n",
        "P \\Big( y_{t+1} , X_{t+1} = j | X_t = i  \\Big) \\\\ \n",
        "&= \\sum_{j=1}^T P\\Big( y_{t+2:T}  |  y_{t+1} , X_{t+1} = j , X_t = i  \\Big) \n",
        "\t\tP \\Big( y_{t+1}|  X_{t+1} = j , X_t = i  \\Big) P \\Big( X_{t+1} = j | X_t = i  \\Big) \\\\ \n",
        "&= \\sum_{j=1}^T  P\\Big( y_{t+2:T} |  X_{t+1} = j  \\Big) P \\Big( y_{t+1} |  X_{t+1} = j \\Big)  P \\Big( X_{t+1} = j | X_t = i  \\Big) \\\\ \n",
        "&= \\sum_{j=1}^T \\beta_j(t+1) b_{j}(y_{t+1}) a_{ij} \\\\ \n",
        "\\end{align}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0-nfIfR7emB"
      },
      "source": [
        "##**Question 2.1** Description du fonctionnement de la fonction d'estimation des paramètres $\\pi$, A et B."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mPLuCWd7Y5G"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def HMC_Parameters_Dict(dataset):\n",
        "        \n",
        "    Pi = {}\n",
        "    A = {}\n",
        "    B = {}\n",
        "        \n",
        "    for Z in dataset:\n",
        "        \n",
        "        Z0 = Z[0]\n",
        "        X0 = Z0[0]\n",
        "        Y0 = Z0[1]\n",
        "        \n",
        "        if X0 not in Pi.keys():\n",
        "            Pi[X0] = 0\n",
        "            \n",
        "        Pi[X0] = Pi[X0] + 1\n",
        "        \n",
        "        if X0 not in B.keys():\n",
        "            B[X0] = {}\n",
        "            \n",
        "        if Y0 not in B[X0].keys():\n",
        "            B[X0][Y0] = 0\n",
        "        \n",
        "        B[X0][Y0] = B[X0][Y0] + 1\n",
        "        \n",
        "        \n",
        "        Zi_prev = Z0\n",
        "        \n",
        "        for Zi in Z[1:]:\n",
        "            x = Zi[0]\n",
        "            y = Zi[1]\n",
        "            \n",
        "            if x not in Pi.keys():\n",
        "                Pi[x] = 0\n",
        "            \n",
        "            if Zi_prev[0] not in A.keys():\n",
        "                A[Zi_prev[0]] = {}\n",
        "                \n",
        "            if x not in A[Zi_prev[0]].keys():\n",
        "                A[Zi_prev[0]][x] = 0\n",
        "                \n",
        "            if x not in B.keys():\n",
        "                B[x] = {}\n",
        "                \n",
        "            if y not in B[x].keys():\n",
        "                B[x][y] = 0\n",
        "                            \n",
        "            Pi[x] = Pi[x] + 1\n",
        "            A[Zi_prev[0]][x] = A[Zi_prev[0]][x] + 1\n",
        "            B[x][y] = B[x][y] + 1\n",
        "            \n",
        "            Zi_prev = Zi\n",
        "            \n",
        "    sum_Pi = np.sum(list(Pi.values()))\n",
        "    for key in Pi.keys():\n",
        "        Pi[key] = Pi[key]/sum_Pi\n",
        "        \n",
        "    for key_A_1 in A.keys():\n",
        "        sum_A = np.sum(list(A[key_A_1].values()))\n",
        "        for key_A_2 in A[key_A_1].keys():\n",
        "            A[key_A_1][key_A_2] = A[key_A_1][key_A_2]/sum_A\n",
        "            \n",
        "    for key_B_1 in B.keys():\n",
        "        sum_B = np.sum(list(B[key_B_1].values()))\n",
        "        for key_B_2 in B[key_B_1].keys():\n",
        "            B[key_B_1][key_B_2] = B[key_B_1][key_B_2]/sum_B\n",
        "    \n",
        "    \n",
        "    return Pi, A, B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgHYUmC_8X97"
      },
      "source": [
        "La fonction ci-dessus prend en paramètre notre dataset qui se présente sous la forme d'une liste python. Afin de simplifier la description, nous allons prenons l'exemple de dataset ci-dessous pour bien expliquer le fonctionnement de notre fonction. $Dataset=[(Noun, Bruce), (Verb, is),(Noun, home)]$.\n",
        "\n",
        "On commence par initialiser nos paramètres $\\pi$, A et B, qui sont des dictionnaires (vides au début).  On parcourt notre dataset avec la boucle for. On définit Z0 comme étant le premier élément de notre dataset ie Z0 est de la forme ($x_1$, $y_1$) où $x_1$ désigne la fonction grammaticale et $y_1$ la variable observée. Avec notre exemple précédent Z0 sera égale à $(Noun, Bruce)$. On affecte ensuite à X0 la fonction grammaticale $x_1$ et à Y0 $y_1$. On vérifie ensuite si la fonction grammaticale X0 ne fait pas partie des clés du dictionnaire $\\pi$. Si c'est le cas, on ajoute cette clé à ce dictionnaire avec la valeur 0. ($\\{\"Noun\": 0\\}$ avec notre exemple illustratif). \n",
        "On incrémente la valeur de clé X0 de $\\pi$, pour dire que ce dernier apparaît au moins une fois..\n",
        "\n",
        "Si la fonction grammitacale X0 n'est pas une clé du dictionnaire B, on l'ajoute avec comme valeur un dictionnaire vide. B est donc un dictionnaire de dictionnaires c'est à dire de la forme $\\{\"key1\": \\{\"key1bis\": value\\}, \\dots\\}$. Value désigne le nombre d'occurences de la clé key1bis. Si le mot observé Y0 n'est pas une clé du dictionnaire correspondant à la valeur de la clé X0 ie une clé du dictionnaire suivant: $\\{\"key1bis\": value\\}$, alors on l'ajoute avec une entrée qui vaut 0. Dans notre exemple $value=0$ et B est de la forme : $\\{\"X0\": \\{\"Y0\": 0\\}, \\dots\\}=\\{\"Noun\": \\{\"Bruce\": 0\\}, \\dots\\}$. On incrémente ensuite pour dire que Y0 apparaît au moins une fois.\n",
        "\n",
        "Le dictionnaire A est aussi un dictionnaire de dictionnaire qui contient des fonctions grammaticales et dans chaque dictionnaire de fonction grammaticale on a une fonction grammaticale et son nombre d'apparition. On affecte à Z_prev Z0 et parcourt à nouveau notre dataset comme précédemment. On définit $x$ et $y$ comme étant respectivement la fonction grammaticale et le mot observé. Si x n'existe pas dans $\\pi$, on le crée avec un nombre d'occurences qui vaut zéro. Dans A, on crée un dictionnaire de la foncton grammaticale précédente et on vérifie si x apparait dans dictionnaire correspondant à la valeur de cette entrée. Si ce n'est pas le cas, on le crée comme dans ça été fait un peu plus haut. \n",
        "\n",
        "A la fin, on normalise afin que les valeurs soient comprises entre 0 et 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsF-LIfeNqaq"
      },
      "source": [
        "##**Question 2.2** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZqSMRIgQTEu"
      },
      "source": [
        "On préfère utiliser les dictionnaires à la place des matrices car la recherche y est beaucoup plus rapide. De plus, les matrices prennent beaucoup de mémoire. Par conséquent pour éviter gaspiller de la mémoire, on utilise les dictionnaires."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY3MrhFzN_ei"
      },
      "source": [
        "##**Question 3** Complétion des codes du fichier HMC_Forword_Backword_Restator.py "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7A2dBHzOEzt"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class HMC_Forward_Backward_Restorator:\n",
        "    \n",
        "    def __init__(self, hmc = False):\n",
        "        self.hmc = hmc\n",
        "        \n",
        "    def forward_prime(self, Omega_X, Y, parameter_set, default = 0):\n",
        "        T = len(Y)\n",
        "        alpha = np.zeros((T, len(Omega_X)))\n",
        "        \n",
        "        Pi = parameter_set[\"Pi\"]\n",
        "        A = parameter_set[\"A\"]\n",
        "        B = parameter_set[\"B\"]\n",
        "\n",
        "        alpha[0] = self.compute_alpha_1(Omega_X, Y, Pi, B, default)\n",
        "        \n",
        "        for t in range(0, T - 1):\n",
        "            alpha[t + 1] = self.compute_alpha_t_plus_1(Omega_X, Y, t + 1, A, B, alpha[t], default)\n",
        "     \n",
        "        return alpha\n",
        "    \n",
        "    \n",
        "    def backward_prime(self, Omega_X, Y, parameter_set, default = 0):\n",
        "        T = len(Y)\n",
        "        beta = np.zeros((T, len(Omega_X)))\n",
        "        \n",
        "        A = parameter_set[\"A\"]\n",
        "        B = parameter_set[\"B\"]\n",
        "        \n",
        "        beta[T - 1] = self.compute_beta_T(Omega_X)\n",
        "        \n",
        "        for t in range(T - 2, -1, -1):\n",
        "            beta[t] = self.compute_beta_t(Omega_X, Y, t + 1, A, B, beta[t + 1], default)\n",
        "        \n",
        "        return beta\n",
        "    \n",
        "    \n",
        "    def compute_gamma(self, alpha, beta):\n",
        "        gamma = alpha * beta\n",
        "    \n",
        "        gamma = np.transpose(gamma)/np.sum(gamma, axis = 1)\n",
        "        gamma = np.transpose(gamma)\n",
        "    \n",
        "        return gamma\n",
        "    \n",
        "    def return_gamma(self, Omega_X, Y, parameter_set, epsilon_laplace = 0):\n",
        "        alpha = self.forward_prime(Omega_X, Y, parameter_set, epsilon_laplace)\n",
        "        beta = self.backward_prime(Omega_X, Y, parameter_set, epsilon_laplace)  \n",
        "        \n",
        "        gamma = self.compute_gamma(alpha, beta)\n",
        "        \n",
        "        return gamma\n",
        "    \n",
        "    def restore_X(self, Omega_X, Y, parameter_set, epsilon_laplace = 0):\n",
        "        alpha = self.forward_prime(Omega_X, Y, parameter_set, epsilon_laplace)\n",
        "        beta = self.backward_prime(Omega_X, Y, parameter_set, epsilon_laplace)  \n",
        "        \n",
        "        gamma = self.compute_gamma(alpha, beta)\n",
        "        \n",
        "        X = []\n",
        "        indexes = np.argmax(gamma, axis = 1)\n",
        "        \n",
        "        for index in indexes:\n",
        "            X.append(Omega_X[index])\n",
        "        \n",
        "        return X\n",
        "    \n",
        "    \n",
        "    #######################\n",
        "    ### COMPUTING ALPHA ###\n",
        "    #######################\n",
        "    \n",
        "    def compute_alpha_1(self, Omega_X, Y, Pi, B, default = 0):\n",
        "      \n",
        "        alpha_1 = np.zeros(len(Omega_X))\n",
        "                \n",
        "        for idi, i in enumerate(Omega_X):\n",
        "            if Y[0] in B[i]:\n",
        "                alpha_1[idi]= B[i][Y[0]]*Pi[i]\n",
        "                    \n",
        "        if np.sum(alpha_1) == 0:\n",
        "            for idi, i in enumerate(Omega_X):\n",
        "                alpha_1[idi]= Pi[i]\n",
        "            \n",
        "        alpha_1 = (alpha_1 + default)/np.sum(alpha_1 + default)\n",
        "                    \n",
        "        return alpha_1\n",
        "    \n",
        "    def compute_alpha_t_plus_1(self, Omega_X, Y, t_plus_1, A, B, alpha_t, default = 0):\n",
        "        alpha_t_plus_1 = np.zeros(len(Omega_X))\n",
        "        \n",
        "        yt_plus_1 = Y[t_plus_1]\n",
        "                \n",
        "        for idi, i in enumerate(Omega_X):\n",
        "            for idj, j in enumerate(Omega_X):\n",
        "            \n",
        "                if j in A and i in B:\n",
        "                    if i in A[j] and yt_plus_1 in B[i]:\n",
        "                        alpha_t_plus_1[idi] += B[i][yt_plus_1]*alpha_t[idj]*A[j][i]\n",
        "        \n",
        "        \n",
        "        if np.sum(alpha_t_plus_1) == 0:\n",
        "            for idi, i in enumerate(Omega_X):\n",
        "                for idj, j in enumerate(Omega_X):\n",
        "                    \n",
        "                    if j in A:\n",
        "                        if i in A[j]:\n",
        "                            alpha_t_plus_1[idi] += alpha_t[idj]*A[j][i]\n",
        "        \n",
        "\n",
        "        alpha_t_plus_1 = (alpha_t_plus_1 + default)/np.sum(alpha_t_plus_1 + default)\n",
        "        return alpha_t_plus_1\n",
        "    \n",
        "    \n",
        "    ######################\n",
        "    ### COMPUTING BETA ###\n",
        "    ######################\n",
        "    \n",
        "    def compute_beta_T(self, Omega_X):\n",
        "        beta_T = np.ones(len(Omega_X))\n",
        "        beta_T = beta_T/np.sum(beta_T)\n",
        "        \n",
        "        return beta_T\n",
        "    \n",
        "    \n",
        "    def compute_beta_t(self, Omega_X, Y, t_plus_1, A, B, beta_t_plus_1, default = 0):\n",
        "        beta_t = np.zeros(len(Omega_X))\n",
        "        \n",
        "        yt_plus_1 = Y[t_plus_1]\n",
        "        \n",
        "        for idi, i in enumerate(Omega_X):\n",
        "            for idj, j in enumerate(Omega_X):\n",
        "                if i in A and j in B:\n",
        "                    if j in A[i] and yt_plus_1 in B[j]:\n",
        "                        beta_t[idi] +=  beta_t_plus_1[idj]*A[i][j]*B[j][yt_plus_1]   \n",
        "                            \n",
        "        if np.sum(beta_t) == 0:\n",
        "            for idi, i in enumerate(Omega_X):\n",
        "                for idj, j in enumerate(Omega_X):\n",
        "                    if i in A:\n",
        "                        if j in A[i]:\n",
        "                            beta_t[idi] += beta_t_plus_1[idj]*A[i][j]\n",
        "        \n",
        "        \n",
        "        beta_t = (beta_t + default)/np.sum(beta_t + default)\n",
        "        return beta_t\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pW7nTnWOI8Y"
      },
      "source": [
        "##**Question 4** Résultats et Commentaires"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-Hc6A7WOn1h"
      },
      "source": [
        "Nous obtenons les résultats ci-dessous. \\\\\n",
        "\n",
        "\\begin{array}{|l|l|l|l|l|} \n",
        "\\hline\n",
        "                       & ~ ~ ~ ~ \\textit{Accuracy} & ~ ~ \\textit{Known Words} & ~~ \\textit{Unknown Words} & ~ \\textit{Training time}  \\\\ \n",
        "\\hline\n",
        "CoNLL2000 POS Tagging  & 5.359140511218513         & 1.9444129325014217       & 50.9388249545572986       & 1.0881872177124023        \\\\ \n",
        "\\hline\n",
        "CoNLL 2003 POS Tagging & 9.912919774108715         & 3.979520031059664        & 57.09048811428516         &                           \\\\ \n",
        "\\hline\n",
        "UD English POS Tagging & 11.161141217723937        & 6.222559767491413        & 65.26917579799905         &                           \\\\ \n",
        "\\hline\n",
        "CoNLL 2000 Chunking    & 7.191253139709147         & 6.677254679523543        & 14.052789642640823        & 1.0331740379333496        \\\\ \n",
        "\\hline\n",
        "CoNLL 2003 Chunking    & 5.62357201362245          & 5.3073475528837831       & 8.9330503356939135        & 1.3438026905059814        \\\\ \n",
        "\\hline\n",
        "HMC F1 CoNLL 2003 Ner  & 0.7249928346231013        & 0.8692506459948321       & 0.011082693947144074      &                           \\\\\n",
        "\\hline\n",
        "\\end{array}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02jGOOoEm9YS"
      },
      "source": [
        "La première observation qu'on peut faire en voyant ce tableau est la différence significative des résultats obtenus pour les mots connus et ceux qui sont inconnus. En effet, on remarque que pour les mots qui ne sont pas présents dans notre dataset d'entrainement (ie inconnus), le taux d'erreur est largement plus grand que celui pour les mots connus. Une explication assez logique à cette différence est que la connaissance d'un mot c'est-à-dire sa présence dans le dataset nous fournit plus d'informations sur les fonctions grammaticales donc sur l'état caché, ce qui n'est pas le cas pour un mot inconnu.\n",
        "\n",
        "Une comparaison entre le Chunking et Pos Tagging montre que le premier présente des résultats plus satisfaisants. Cette performance pourrait s'expliquer par le fait que le chunking comme on l'a vu dans la partie théorique de ce TP est à un cran au dessus du Pos Tagging. En effet, dans le POS Tagging, on labélise chaque mot d'une phrase alors que pour le chunking on s'intéresse au groupe de mots. Pour des mots composés comme \"South Africa\" par exemple, le Pos Tagging peut donner lieu à des erreurs. \n",
        "\n",
        "La performance du chunking est plus intéressante pour les mots inconnus. A tire d'exemple, on a une erreur d'environ 51% pour les mots inconnus dans le cas du Pos Tagging (CoNLL 2000) contre un taux d'erreur de de 14%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzGcwSxOg8dq"
      },
      "source": [
        "##**Question 5** Normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dw1-5EyhNcq"
      },
      "source": [
        "On normalise pour éviter certaines erreurs qui peuvent être dues à des faibles valeurs des probas forward et backward. La normalisation permet donc de se prémunir de ces erreurs. Elle n'affecte pas les résultats de notre algorithme. Considérons la formule ci-dessous:\n",
        "\n",
        "\\begin{align}\n",
        "P(X_t=i|y_{1:T})=\\frac{\\alpha_i(t)\\beta_i(t)}{\\sum_{j \\in \\Omega_X}\\alpha_j(t)\\beta_j(t)}\n",
        "\\end{align}\n",
        "Normaliser les probas forward et backward revient à diviser le numérateur et le dénominateur des probabilités à posteriori par la même valeur, ce qui ne change rien à cette quantité."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQu22Lcz0UY0"
      },
      "source": [
        "##**Question 6** Utilité de epsilon_laplace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VtP2f090wFY"
      },
      "source": [
        "Ce paramètre sert à mesurer la confiance qu'on a des résultats que nous avons trouvés pour les calculs de nos probabilités forward et backward. Dans notre code, il est égale à 0, ce qui se traduit par une confiance $\\textit{\"absolue\"}$ en ceux-ci.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKkaudUy2QBJ"
      },
      "source": [
        "##**Question 7** Amélioration des performances de notre algorithme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDg2Vs0410Oi"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class HMC_Down_Forward_Backward_Restorator:\n",
        "    \n",
        "    def __init__(self, hmc = False):\n",
        "        self.hmc = hmc\n",
        "        \n",
        "    def forward_prime(self, Omega_X, list_Y, parameter_set, default = 0):\n",
        "        T = len(list_Y[0])\n",
        "        alpha = np.zeros((T, len(Omega_X)))\n",
        "        \n",
        "        Pi_hmc = parameter_set[\"Pi_HMC\"]\n",
        "        A_hmc = parameter_set[\"A_HMC\"]\n",
        "        list_B_hmc = parameter_set[\"list_B_HMC\"]\n",
        "\n",
        "        alpha[0] = self.compute_alpha_1(Omega_X, list_Y, Pi_hmc, list_B_hmc, default)\n",
        "        \n",
        "        for t in range(0, T - 1):\n",
        "            alpha[t + 1] = self.compute_alpha_t_plus_1(Omega_X, list_Y, t + 1, A_hmc, list_B_hmc, alpha[t], default)\n",
        "     \n",
        "        return alpha\n",
        "    \n",
        "    \n",
        "    def backward_prime(self, Omega_X, list_Y, parameter_set, default = 0):\n",
        "        T = len(list_Y[0])\n",
        "        beta = np.zeros((T, len(Omega_X)))\n",
        "        \n",
        "        A_hmc = parameter_set[\"A_HMC\"]\n",
        "        list_B_hmc = parameter_set[\"list_B_HMC\"]\n",
        "        \n",
        "        beta[T - 1] = self.compute_beta_T(Omega_X)\n",
        "        \n",
        "        for t in range(T - 2, -1, -1):\n",
        "            beta[t] = self.compute_beta_t(Omega_X, list_Y, t + 1, A_hmc, list_B_hmc, beta[t + 1], default)\n",
        "        \n",
        "        return beta\n",
        "    \n",
        "    \n",
        "    def compute_gamma(self, alpha, beta):\n",
        "        gamma = alpha * beta\n",
        "    \n",
        "        gamma = np.transpose(gamma)/np.sum(gamma, axis = 1)\n",
        "        gamma = np.transpose(gamma)\n",
        "    \n",
        "        return gamma\n",
        "    \n",
        "    def return_gamma(self, Omega_X, Y, parameter_set, epsilon_laplace = 0):\n",
        "        alpha = self.forward_prime(Omega_X, Y, parameter_set, epsilon_laplace)\n",
        "        beta = self.backward_prime(Omega_X, Y, parameter_set, epsilon_laplace)  \n",
        "        \n",
        "        gamma = self.compute_gamma(alpha, beta)\n",
        "        \n",
        "        return gamma\n",
        "    \n",
        "    def restore_X(self, Omega_X, Y, parameter_set, epsilon_laplace = 0):\n",
        "        alpha = self.forward_prime(Omega_X, Y, parameter_set, epsilon_laplace)\n",
        "        beta = self.backward_prime(Omega_X, Y, parameter_set, epsilon_laplace)  \n",
        "        \n",
        "        gamma = self.compute_gamma(alpha, beta)\n",
        "        \n",
        "        X = []\n",
        "        indexes = np.argmax(gamma, axis = 1)\n",
        "        \n",
        "        for index in indexes:\n",
        "            X.append(Omega_X[index])\n",
        "        \n",
        "        return X\n",
        "    \n",
        "    \n",
        "    #######################\n",
        "    ### COMPUTING ALPHA ###\n",
        "    #######################\n",
        "    \n",
        "    def compute_alpha_1(self, Omega_X, list_Y, Pi_hmc, list_B_hmc, default = 0):\n",
        "        alpha_1 = np.zeros(len(Omega_X))\n",
        "        for idi, i in enumerate(Omega_X):\n",
        "            if list_Y[0][0] in list_B_hmc[0][i]:\n",
        "                alpha_1[idi]  = Pi_hmc[i]*list_B_hmc[0][i][list_Y[0][0]]\n",
        "                \n",
        "        \n",
        "        if np.sum(alpha_1) == 0:\n",
        "             for idi, i in enumerate(Omega_X):\n",
        "                \n",
        "                 if list_Y[1][0] in list_B_hmc[1][i]:\n",
        "                     alpha_1[idi] = Pi_hmc[i]*list_B_hmc[1][i][list_Y[1][0]]\n",
        "                     \n",
        "        if np.sum(alpha_1) == 0:\n",
        "            for idi, i in enumerate(Omega_X):\n",
        "               alpha_1[idi]=Pi_hmc[i]\n",
        "            \n",
        "        # to complete\n",
        "            \n",
        "        alpha_1 = (alpha_1 + default)/np.sum(alpha_1 + default)\n",
        "                    \n",
        "        return alpha_1\n",
        "    \n",
        "    def compute_alpha_t_plus_1(self, Omega_X, list_Y, t_plus_1, A_hmc, list_B_hmc, alpha_t, default = 0):\n",
        "        alpha_t_plus_1 = np.zeros(len(Omega_X))\n",
        "        \n",
        "        \n",
        "        yt_plus_1 = list_Y[0][t_plus_1]\n",
        "        yft_plus_1 = list_Y[1][t_plus_1]\n",
        "            \n",
        "        for idi, i in enumerate(Omega_X):\n",
        "            for idj, j in enumerate(Omega_X):\n",
        "            \n",
        "                if j in A_hmc and i in list_B_hmc[0]:\n",
        "                    if i in A_hmc[j] and yt_plus_1 in list_B_hmc[0][i]:\n",
        "                        alpha_t_plus_1[idi] += list_B_hmc[0][i][yt_plus_1]*A_hmc[j][i]*alpha_t[idj]\n",
        "                        \n",
        "        if np.sum(alpha_t_plus_1) == 0:\n",
        "             \n",
        "             for idi, i in enumerate(Omega_X):\n",
        "                 for idj, j in enumerate(Omega_X):\n",
        "                     if j in A_hmc and i in list_B_hmc[1]:\n",
        "                            if i in A_hmc[j] and yft_plus_1 in list_B_hmc[1][i]:\n",
        "                                alpha_t_plus_1[idi] += list_B_hmc[1][i][yft_plus_1]*A_hmc[j][i]*alpha_t[idj]\n",
        "    \n",
        "        if np.sum(alpha_t_plus_1) == 0:\n",
        "            for idi, i in enumerate(Omega_X):\n",
        "                for idj, j in enumerate(Omega_X):\n",
        "                    \n",
        "                    if j in A_hmc:\n",
        "                        if i in A_hmc[j]:\n",
        "                            alpha_t_plus_1[idi] += A_hmc[j][i]*alpha_t[idj]\n",
        "        \n",
        "        \n",
        "        # to complete\n",
        "\n",
        "        alpha_t_plus_1 = (alpha_t_plus_1 + default)/np.sum(alpha_t_plus_1 + default)\n",
        "        return alpha_t_plus_1\n",
        "    \n",
        "    \n",
        "    ######################\n",
        "    ### COMPUTING BETA ###\n",
        "    ######################\n",
        "    \n",
        "    def compute_beta_T(self, Omega_X):\n",
        "        # to complete\n",
        "        beta_T = np.ones(len(Omega_X)) \n",
        "        beta_T = beta_T/np.sum(beta_T)\n",
        "        return beta_T\n",
        "    \n",
        "    \n",
        "    def compute_beta_t(self, Omega_X, list_Y, t_plus_1, A_hmc, list_B_hmc, beta_t_plus_1, default = 0):\n",
        "        beta_t = np.zeros(len(Omega_X))\n",
        "        \n",
        "        yt_plus_1 = list_Y[0][t_plus_1]\n",
        "        yft_plus_1 = list_Y[1][t_plus_1]\n",
        "        \n",
        "        for idi, i in enumerate(Omega_X):\n",
        "            for idj, j in enumerate(Omega_X):\n",
        "                # to complete\n",
        "                if i in A_hmc and j in list_B_hmc[0]:\n",
        "                    if j in A_hmc[i] and yt_plus_1 in list_B_hmc[0][j]:\n",
        "                         beta_t[idi] += beta_t_plus_1[idj]*A_hmc[i][j]*list_B_hmc[0][j][yt_plus_1]\n",
        "                         \n",
        "        if np.sum(beta_t) == 0:\n",
        "             for idi, i in enumerate(Omega_X):\n",
        "                 for idj, j in enumerate(Omega_X):\n",
        "                     if i in A_hmc and j in list_B_hmc[1]:\n",
        "                         if j in A_hmc[i] and yft_plus_1 in list_B_hmc[1][j]:\n",
        "                             beta_t[idi] += beta_t_plus_1[idj]*A_hmc[i][j]*list_B_hmc[1][j][yft_plus_1]\n",
        "                                  \n",
        "        if np.sum(beta_t) == 0:\n",
        "            for idi, i in enumerate(Omega_X):\n",
        "                for idj, j in enumerate(Omega_X):\n",
        "                    # to complete\n",
        "                    if i in A_hmc :\n",
        "                        if j in A_hmc[i]:\n",
        "                            beta_t[idi] += beta_t_plus_1[idj]*A_hmc[i][j]   \n",
        "        \n",
        "        # to complete\n",
        "        beta_t = (beta_t + default)/np.sum(beta_t + default)\n",
        "        return beta_t\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWxmGP-z3Yw5"
      },
      "source": [
        "###Résultas\n",
        "\n",
        "\\begin{array}{|l|l|l|l|l|} \n",
        "\\hline\n",
        "                       & ~ ~ ~ ~ \\textit{Accuracy} & ~ ~ \\textit{Known Words} & ~~ \\textit{Unknown Words} & ~ \\textit{Training time}  \\\\ \n",
        "\\hline\n",
        "CoNLL2000 POS Tagging  & 3.4278236274985687         & 1.9353374929098095       & 23.34948516050879       & 2.2617568969726562        \\\\ \n",
        "\\hline\n",
        "CoNLL 2003 POS Tagging &  6.2464973919041284         & 4.035330372958683        & 23.827898900250815         &  2.8601086139678955                       \\\\ \n",
        "\\hline\n",
        "UD English POS Tagging & 8.997449792795663       & 6.139931295386361        & 40.30490709861839         &  2.6570358276367188                         \\\\ \n",
        "\\hline\n",
        "CoNLL 2000 Chunking    & 7.0878274268104775         & 6.679523539421439        & 12.537855844942456       &         \\\\ \n",
        "\\hline\n",
        "CoNLL 2003 Chunking    & 5.5028667500107815          & 5.209774089442149       &  7.83330117692455        & 2.5722577571868896       \\\\ \n",
        "\\hline\n",
        "HMC F1 CoNLL 2003 Ner  & 0.7773382671940794        & 0.8708512467755803        & 0.5084075173095944     &  2.5655274391174316                      \\\\\n",
        "\\hline\n",
        "\\end{array}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMJK-g5Zckdk"
      },
      "source": [
        "Nous obtenons de meilleurs à résultats à cette question et ce pour tous les tests. On constate ici que le taux d'erreur pour le mots inconnus a considérablement diminué. En effet, nous sommes passés respectivement de 50.93%, 57.09% et 65.26% à 23.34%, 23.82% et 40% pour CoNLL2000PosTagging, CoNLL2003PosTagging et UD_English_PosTagging. La prise en compte des $\\textit{features}$ a ainsi permis d'améliorer de manière très significative les performances de notre modèle. En effet, nous avons pu obtenir des résultats satisfaisants pour des mots ne sont trouvant pas dans notre dataset d'entrainement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsaG0GqagOBk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}